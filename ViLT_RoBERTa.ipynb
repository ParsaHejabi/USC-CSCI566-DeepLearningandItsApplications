{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    ViltProcessor,\n",
    "    ViltModel,\n",
    "    ViltConfig,\n",
    "    AutoTokenizer,\n",
    "    RobertaModel,\n",
    ")\n",
    "import requests\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "# VQA folder path\n",
    "vqa_path = os.path.join(cwd, \"VQA\")\n",
    "# Add VQA folder to path\n",
    "sys.path.append(vqa_path)\n",
    "sys.path.append(os.path.join(vqa_path, \"PythonEvaluationTools\"))\n",
    "sys.path.append(os.path.join(vqa_path, \"PythonHelperTools\"))\n",
    "\n",
    "from vqaEvaluation import vqaEval\n",
    "from vqaTools import vqa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"\"\n",
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\n",
    "            \"MPS not available because the current PyTorch install was not \"\n",
    "            \"built with MPS enabled.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "            \"and/or you do not have an MPS-enabled device on this machine.\"\n",
    "        )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the ViLT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vilt_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "vilt_config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "vilt_model = ViltModel.from_pretrained(\n",
    "    \"dandelin/vilt-b32-finetuned-vqa\",\n",
    "    num_labels=len(vilt_config.id2label),\n",
    "    id2label=vilt_config.id2label,\n",
    "    label2id=vilt_config.label2id,\n",
    ")\n",
    "vilt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image + question\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"Where the cats are sleeping?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "encoding = vilt_processor(image, text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "print(f\"Encoding keys: {encoding.keys()}\")\n",
    "print(f\"Encoding shape: {encoding['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "outputs = vilt_model(**encoding)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(f\"last_hidden_states shape: {last_hidden_states.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Multimodal-Fatima/VQAv2_validation\")\n",
    "dataset[\"validation\"][0].keys()\n",
    "print(f\"Length of dataset: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_DATASET_SIZE = len(dataset[\"validation\"]) // 10\n",
    "sampled_dataset_generator = torch.Generator().manual_seed(42)\n",
    "sampled_dataset_split = torch.utils.data.random_split(\n",
    "    dataset[\"validation\"],\n",
    "    [SAMPLED_DATASET_SIZE, len(dataset[\"validation\"]) - SAMPLED_DATASET_SIZE],\n",
    "    generator=sampled_dataset_generator,\n",
    ")\n",
    "sampled_dataset = {\n",
    "    \"train\": sampled_dataset_split[1],\n",
    "    \"test\": sampled_dataset_split[0],\n",
    "}\n",
    "print(f\"Train size: {len(sampled_dataset['train'])}\")\n",
    "print(f\"Test size: {len(sampled_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset[\"train\"][0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(count: int) -> float:\n",
    "    return min(1.0, count / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor, split):\n",
    "        self.dataset = dataset[split]\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.dataset[idx][\"question\"]\n",
    "        image = self.dataset[idx][\"image\"].convert(\"RGB\")\n",
    "        answers = self.dataset[idx][\"answers\"]\n",
    "\n",
    "        answer_counts = {}\n",
    "        for answer in answers:\n",
    "            answer_counts[answer] = answer_counts.get(answer, 0) + 1\n",
    "\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for answer in answer_counts:\n",
    "            if answer not in list(vilt_config.label2id.keys()):\n",
    "                continue\n",
    "            labels.append(vilt_config.label2id[answer])\n",
    "            scores.append(get_score(answer_counts[answer]))\n",
    "\n",
    "        encoding = self.processor(\n",
    "            image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        for key, value in encoding.items():\n",
    "            encoding[key] = value.squeeze(0)\n",
    "\n",
    "        targets = torch.zeros(len(vilt_config.label2id))\n",
    "        for label, score in zip(labels, scores):\n",
    "            targets[label] = score\n",
    "\n",
    "        encoding[\"labels\"] = targets\n",
    "        encoding[\"caption\"] = self.dataset[idx][\"blip_caption\"]\n",
    "        encoding[\"question_id\"] = self.dataset[idx][\"question_id\"]\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_dataset_train = VQADataset(sampled_dataset, vilt_processor, \"train\")\n",
    "vqa_dataset_test = VQADataset(sampled_dataset, vilt_processor, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    token_type_ids = [item[\"token_type_ids\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    captions = [item[\"caption\"] for item in batch]\n",
    "    question_id = [item[\"question_id\"] for item in batch]\n",
    "\n",
    "    # create padded pixel values and corresponding pixel mask\n",
    "    encoding = vilt_processor.feature_extractor.pad_and_create_pixel_mask(\n",
    "        pixel_values, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # create new batch\n",
    "    batch = {}\n",
    "    batch[\"input_ids\"] = torch.stack(input_ids)\n",
    "    batch[\"attention_mask\"] = torch.stack(attention_mask)\n",
    "    batch[\"token_type_ids\"] = torch.stack(token_type_ids)\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = torch.stack(labels)\n",
    "    batch[\"caption\"] = captions\n",
    "    batch[\"question_id\"] = question_id\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    vqa_dataset_train,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    # num_workers=4\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    vqa_dataset_test,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    # num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[\"pixel_values\"].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "roberta_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.LayerNorm(hidden_size))\n",
    "        layers.append(nn.GELU())\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.GELU())\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1536  # vilt_pooled_size + roberta_pooled_size\n",
    "hidden_size = input_size * 2  # or any other suitable size\n",
    "output_size = 3129  # number of answer classes\n",
    "num_layers = 3  # or any other suitable number of layers\n",
    "\n",
    "decoder = Decoder(input_size, hidden_size, output_size, num_layers).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vilt_to_vqa_annotation(vilt_data):\n",
    "    dummy_annotate = \"\"\"{\"info\": {\"description\": \"This is Balanced Binary Abstract Scenes VQA dataset.\", \n",
    "      \"url\": \"http://visualqa.org\", \n",
    "      \"version\": \"1.0\", \n",
    "      \"year\": \"2017\", \n",
    "      \"contributor\": \"VQA Team\", \n",
    "      \"date_created\": \"2017-03-09 14:27:27\"}, \n",
    "      \"license\": {\"url\": \"http://creativecommons.org/licenses/by/4.0/\", \n",
    "      \"name\": \"Creative Commons Attribution 4.0 International License\"}, \n",
    "      \"data_subtype\": \"val2017\"}\"\"\"\n",
    "\n",
    "    annotate_full = json.loads(dummy_annotate)\n",
    "\n",
    "    dummy_questions = \"\"\"\n",
    "    {\"info\": {\"description\": \"This is v1.0 of the VQA dataset.\", \n",
    "    \"url\": \"http://visualqa.org\", \n",
    "    \"version\": \"1.0\", \n",
    "    \"year\": 2015, \"contributor\": \n",
    "    \"VQA Team\", \"date_created\": \"2015-10-02 19:50:36\"}, \n",
    "    \"task_type\": \"Open-Ended\", \n",
    "    \"data_type\": \"abstract_v002\", \n",
    "    \"license\": {\"url\": \"http://creativecommons.org/licenses/by/4.0/\", \n",
    "      \"name\": \"Creative Commons Attribution 4.0 International License\"}, \n",
    "    \"data_subtype\": \"val2015\"}\n",
    "    \"\"\"\n",
    "\n",
    "    question_full = json.loads(dummy_questions)\n",
    "\n",
    "    annotate_list = []\n",
    "\n",
    "    question_list = []\n",
    "\n",
    "    for datapoint in vilt_data:\n",
    "        \"\"\"\n",
    "        print('========')\n",
    "        for key in datapoint.keys():\n",
    "          print(key)\n",
    "        print('========')\n",
    "        \"\"\"\n",
    "\n",
    "        # VILT converted VQA keys, so we need to convert them back\n",
    "        datapoint[\"image_id\"] = datapoint[\"id_image\"]\n",
    "        datapoint[\"answers\"] = datapoint[\"answers_original\"]\n",
    "        # Throw away unneeded stuff\n",
    "        datapoint[\"image\"] = None\n",
    "        datapoint[\"LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14\"] = None\n",
    "        datapoint[\"DETA_detections_deta_swin_large_o365_coco_classes\"] = None\n",
    "        datapoint[\"DETA_detections_deta_swin_large_o365_clip_ViT_L_14\"] = None\n",
    "        datapoint[\n",
    "            \"DETA_detections_deta_swin_large_o365_clip_ViT_L_14_blip_caption\"\n",
    "        ] = None\n",
    "\n",
    "        question_item = {\n",
    "            \"question_id\": datapoint[\"question_id\"],\n",
    "            \"image_id\": datapoint[\"image_id\"],\n",
    "            \"question\": datapoint[\"question\"],\n",
    "        }\n",
    "\n",
    "        question_list.append(question_item)\n",
    "        annotate_list.append(datapoint)\n",
    "\n",
    "    annotate_full[\"annotations\"] = annotate_list\n",
    "    question_full[\"questions\"] = question_list\n",
    "\n",
    "    return {\"annotations\": annotate_full, \"questions\": question_full}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_EVERY = 10\n",
    "EPOCHS = 15\n",
    "\n",
    "epoch_tqdm_bar = tqdm(range(EPOCHS), desc=\"Epoch\")\n",
    "for epoch in epoch_tqdm_bar:\n",
    "    epoch_tqdm_bar.set_description(f\"Epoch {epoch}\")\n",
    "    batch_tqdm_bar = tqdm(train_dataloader, desc=\"Batch\")\n",
    "\n",
    "    decoder.train()\n",
    "    total_train_loss = 0\n",
    "    for i, batch in enumerate(batch_tqdm_bar, 1):\n",
    "        batch_tqdm_bar.set_description(f\"Batch {i}\")\n",
    "        caption = batch.pop(\"caption\")\n",
    "        labels = batch.pop(\"labels\").to(device)\n",
    "        question_id = batch.pop(\"question_id\")\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        vilt_output = vilt_model(**batch)\n",
    "        vilt_pooled_output = vilt_output.pooler_output\n",
    "\n",
    "        roberta_tokenized_captions = roberta_tokenizer(\n",
    "            caption, return_tensors=\"pt\", padding=True\n",
    "        ).to(device)\n",
    "        roberta_output = roberta_model(**roberta_tokenized_captions)\n",
    "        roberta_pooled_output = roberta_output.pooler_output\n",
    "\n",
    "        concatenated_output = torch.cat(\n",
    "            (vilt_pooled_output, roberta_pooled_output), dim=-1\n",
    "        )\n",
    "\n",
    "        logits = decoder(concatenated_output)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        if i % PRINT_EVERY == 0:\n",
    "            avg_train_loss = total_train_loss / PRINT_EVERY\n",
    "            print(f\"  Batch {i}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "            total_train_loss = 0\n",
    "\n",
    "    torch.save(decoder.state_dict(), f\"decoder_model_epoch_{epoch}.pth\")\n",
    "\n",
    "    # Evaluation loop for one epoch\n",
    "    decoder.eval()\n",
    "    results = []\n",
    "    answers = []\n",
    "    with torch.no_grad():\n",
    "        test_batch_tqdm_bar = tqdm(test_dataloader, desc=\"Test Batch\")\n",
    "        for batch in test_batch_tqdm_bar:\n",
    "            test_batch_tqdm_bar.set_description(f\"Test Batch {i}\")\n",
    "            caption = batch.pop(\"caption\")\n",
    "            labels = batch.pop(\"labels\").to(device)\n",
    "            question_id = batch.pop(\"question_id\")\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            vilt_output = vilt_model(**batch)\n",
    "            vilt_pooled_output = vilt_output.pooler_output\n",
    "\n",
    "            roberta_tokenized_captions = roberta_tokenizer(\n",
    "                caption, return_tensors=\"pt\", padding=True\n",
    "            ).to(device)\n",
    "            roberta_output = roberta_model(**roberta_tokenized_captions)\n",
    "            roberta_pooled_output = roberta_output.pooler_output\n",
    "\n",
    "            concatenated_output = torch.cat(\n",
    "                (vilt_pooled_output, roberta_pooled_output), dim=-1\n",
    "            )\n",
    "\n",
    "            logits = decoder(concatenated_output)\n",
    "\n",
    "            for i in range(len(logits)):\n",
    "                idx = logits[i].argmax(-1).item()\n",
    "                answer = vilt_model.config.id2label[idx]\n",
    "                answers.append(answer)\n",
    "\n",
    "                results.append({\"question_id\": question_id[i], \"answer\": answer})\n",
    "\n",
    "        convert_data = vilt_to_vqa_annotation(sampled_dataset[\"test\"])\n",
    "\n",
    "        convert_annotate, convert_questions = (\n",
    "            convert_data[\"annotations\"],\n",
    "            convert_data[\"questions\"],\n",
    "        )\n",
    "\n",
    "        with open(f\"./annotate_epoch_{epoch}.json\", \"w\") as annotate_file:\n",
    "            annotate_file.write(json.dumps(convert_annotate))\n",
    "\n",
    "        with open(f\"./questions_epoch_{epoch}.json\", \"w\") as annotate_file:\n",
    "            annotate_file.write(json.dumps(convert_questions))\n",
    "\n",
    "        with open(f\"./results_epoch_{epoch}.json\", \"w\") as res_file:\n",
    "            res_file.write(json.dumps(results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved decoder model\n",
    "# loaded_decoder = Decoder(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "# loaded_decoder.load_state_dict(torch.load(model_save_path))\n",
    "# loaded_decoder.eval()  # Set the model to evaluation mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
